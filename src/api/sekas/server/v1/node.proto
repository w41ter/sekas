// Copyright 2023-present The Sekas Authors.
// Copyright 2022 The Engula Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package sekas.server.v1;

import "sekas/server/v1/error.proto";
import "sekas/server/v1/metadata.proto";
import "sekas/server/v1/types.proto";
import "sekas/server/v1/write.proto";
import "google/protobuf/field_mask.proto";

service Node {
    rpc Batch(BatchRequest) returns (BatchResponse) {}
    /// A set methods to manipulate node.
    rpc Admin(NodeAdminRequest) returns (NodeAdminResponse) {}
    /// A set methods about shard moving.
    rpc MoveShard(MoveShardRequest) returns (MoveShardResponse) {}
}

message BatchRequest {
    uint64 node_id = 1;
    repeated GroupRequest requests = 2;
}

message BatchResponse { repeated GroupResponse responses = 1; }

message GroupRequest {
    uint64 group_id = 1;
    uint64 epoch = 2;
    GroupRequestUnion request = 3;
}

message GroupResponse {
    GroupResponseUnion response = 1;

    /// Only used in BatchResponse.
    Error error = 2;
}

message GroupRequestUnion {
    oneof request {
        // Single operation.
        ShardGetRequest get = 1;
        ShardScanRequest scan = 2;
        ShardWriteRequest write = 3;

        // Txn releated operation.
        WriteIntentRequest write_intent = 4;
        CommitIntentRequest commit_intent = 5;
        ClearIntentRequest clear_intent = 6;

        /// Add a new shard to an existing group.
        CreateShardRequest create_shard = 7;

        /// Change replicas of an existing group.
        ChangeReplicasRequest change_replicas = 8;

        /// Issue a moving shard request. This request need to send to the dest group.
        AcceptShardRequest accept_shard = 9;

        /// Transfer leadership to another replicas. This request is executed as a notice, so
        /// the response doesn't reflect the actual execution result of transferring.
        TransferRequest transfer = 10;

        /// MoveReplicas delegates the replicas moving to group leader.
        ///
        /// Response once the group leader accepts the moving replicas request. When there exists
        /// some conflicts, such as group is in joint, `Error::AlreadyExists` is returned.
        MoveReplicasRequest move_replicas = 11;
    }
}

message GroupResponseUnion {
    oneof response {
        ShardGetResponse get = 1;
        ShardScanResponse scan = 2;
        ShardWriteResponse write = 3;
        WriteIntentResponse write_intent = 4;
        CommitIntentResponse commit_intent = 5;
        ClearIntentResponse clear_intent = 6;

        CreateShardResponse create_shard = 7;
        ChangeReplicasResponse change_replicas = 8;
        AcceptShardResponse accept_shard = 9;
        TransferResponse transfer = 10;
        MoveReplicasResponse move_replicas = 11;
    }
}

/// The batch writes to a shard which ensure atomic writes.
message ShardWriteRequest {
    uint64 shard_id = 1;
    repeated DeleteRequest deletes = 2;
    repeated PutRequest puts = 3;
}

/// The response of batch writes to a shard.
message ShardWriteResponse {
    repeated WriteResponse deletes = 1;
    repeated WriteResponse puts = 2;
}

message ShardGetRequest {
    uint64 shard_id = 1;
    uint64 start_version = 2;
    bytes user_key = 3;
}

message ShardGetResponse {
    optional Value value = 1;
}

message ShardScanRequest {
    /// The id of target shard.
    uint64 shard_id = 1;
    /// The start version of scan request.
    uint64 start_version = 2;
    /// The maxminum key-values pairs to fetch, 0 means no limit.
    uint64 limit = 3;
    /// The maxminum key-value bytes to fetch, 0 means no limit.
    uint64 limit_bytes = 4;
    /// Shoud include start key in result? not support in prefix scan.
    bool exclude_start_key = 5;
    /// Shoud include end key in result? not support in prefix scan.
    bool exclude_end_key = 6;
    /// Scan with prefix, if set then `start_key` and `end_key` will be ignored.
    ///
    /// The prefix only works with range shard, the behaviour of hash shard is undefined.
    optional bytes prefix = 7;
    /// The start key for a hash shard will be locate with hash slot.
    optional bytes start_key = 8;
    /// The end key only works with range shard, the behaviour of hash shard is undefined.
    optional bytes end_key = 9;
    /// Include all data (tombstone) as return values.
    bool include_raw_data = 10;
    /// Ignore txn intents rather than resolve it.
    bool ignore_txn_intent = 11;
    /// Allow scan an moving shard, without forwarding.
    bool allow_scan_moving_shard = 12;
}

message ShardScanResponse {
    /// The value set.
    repeated ValueSet data = 1;
    /// Has more data to scan?
    bool has_more = 2;
}

message WriteIntentRequest {
    uint64 shard_id = 1;
    uint64 start_version = 2;

    /// The write request.
    oneof write {
        DeleteRequest delete = 3;
        PutRequest put = 4;
    }
}

message WriteIntentResponse {
    WriteResponse write = 1;
}

message CommitIntentRequest {
    uint64 shard_id = 1;
    uint64 start_version = 2;
    uint64 commit_version = 3;
    bytes user_key = 4;
}

message CommitIntentResponse {}

message ClearIntentRequest {
    uint64 shard_id = 1;
    uint64 start_version = 2;
    bytes user_key = 3;
}

message ClearIntentResponse {}

message NodeAdminRequest {
    oneof request {
        GetRootRequest get_root = 1;
        CreateReplicaRequest create_replica = 2;

        /// RemoveReplica allows shuts down and deletes an orphan replica from the
        /// specified node.
        ///
        /// It is only executed when the user specifies a newer `GroupDesc` and the
        /// replica no longer belongs to the group.
        RemoveReplicaRequest remove_replica = 3;
        HeartbeatRequest heartbeat = 4;
    }
}

message NodeAdminResponse {
    oneof response {
        GetRootResponse get_root = 1;
        CreateReplicaResponse create_replica = 2;
        RemoveReplicaResponse remove_replica = 3;
        HeartbeatResponse heartbeat = 4;
    }
}

message GetRootRequest {}

message GetRootResponse { RootDesc root = 1; }

message CreateReplicaRequest {
    uint64 replica_id = 1;
    GroupDesc group = 2;
}

message CreateReplicaResponse {}

message RemoveReplicaRequest {
    uint64 replica_id = 1;
    GroupDesc group = 2;
}

message RemoveReplicaResponse {}

message CreateShardRequest { ShardDesc shard = 1; }

message CreateShardResponse {}

message ChangeReplicasRequest { ChangeReplicas change_replicas = 1; }

message ChangeReplicasResponse {}

message ChangeReplicas { repeated ChangeReplica changes = 1; }

message ChangeReplica {
    ChangeReplicaType change_type = 1;

    uint64 replica_id = 2;
    uint64 node_id = 3;
}

enum ChangeReplicaType {
    ADD = 0;
    REMOVE = 1;
    ADD_LEARNER = 2;
}

message AcceptShardRequest {
    /// The source group of this moving shard.
    uint64 src_group_id = 1;
    /// The epoch of source group when issuing this moving shard request.
    uint64 src_group_epoch = 2;
    /// The descriptor of moving shard.
    ShardDesc shard_desc = 3;
}

message AcceptShardResponse {}

message TransferRequest {
    uint64 transferee = 1;
}

message TransferResponse {}

message HeartbeatRequest {
    uint64 timestamp = 1;
    repeated PiggybackRequest piggybacks = 2;
}

message HeartbeatResponse {
    uint64 timestamp = 1;
    /// The epoch of root group which contained in node's `RootDesc`.
    uint64 root_epoch = 2;
    repeated PiggybackResponse piggybacks = 3;
}

message PiggybackRequest {
    oneof info {
        SyncRootRequest sync_root = 1;
        CollectStatsRequest collect_stats = 2;
        CollectGroupDetailRequest collect_group_detail = 3;
        CollectScheduleStateRequest collect_schedule_state = 4;
        CollectMovingShardStateRequest collect_moving_shard_state = 5;
    }
}

message PiggybackResponse {
    oneof info {
        SyncRootResponse sync_root = 1;
        CollectStatsResponse collect_stats = 2;
        CollectGroupDetailResponse collect_group_detail = 3;
        CollectScheduleStateResponse collect_schedule_state = 4;
        CollectMovingShardStateResponse collect_moving_shard_state = 5;
    }
}

message SyncRootRequest { RootDesc root = 1; }

message SyncRootResponse {}

message CollectStatsRequest { google.protobuf.FieldMask field_mask = 1; }

message CollectStatsResponse {
    NodeStats node_stats = 1;
    repeated GroupStats group_stats = 2;
    repeated ReplicaStats replica_stats = 3;
}

message NodeStats {
    uint64 available_space = 1;
    uint32 group_count = 2;
    uint32 leader_count = 3;
    /// The replicas field in `GroupDesc` is empty.
    uint64 orphan_replica_count = 4;
    float read_qps = 5;
    float write_qps = 6;
}

message GroupStats {
    uint64 group_id = 1;
    uint64 shard_count = 2;
    float read_qps = 3;
    float write_qps = 4;
}

message ReplicaStats {
    uint64 replica_id = 1;
    uint64 group_id = 2;
    float read_qps = 3;
    float write_qps = 4;
}

message CollectGroupDetailRequest {
    /// The ID list of the group that needs to get the status, if it is empty, get
    /// all the groups on the target machine.
    repeated uint64 groups = 1;
}

message CollectGroupDetailResponse {
    repeated ReplicaState replica_states = 1;
    /// If a replica is the leader of group, it also needs to be responsible for
    /// filling in the `GroupDesc`.
    repeated GroupDesc group_descs = 2;
}

message CollectScheduleStateRequest {}

message CollectScheduleStateResponse {
    repeated ScheduleState schedule_states = 1;
}

message CollectMovingShardStateRequest { uint64 group = 1; }

message CollectMovingShardStateResponse {
    enum State {
        NONE = 0;
        PREPARE = 1;
        MOVING = 2;
        MOVED = 3;
    }

    State state = 1;
    MoveShardDesc desc = 2;
}

message MoveReplicasRequest {
    repeated ReplicaDesc incoming_voters = 1;
    repeated ReplicaDesc outgoing_voters = 2;
}

message MoveReplicasResponse {
    ScheduleState schedule_state = 1;
}

message MoveShardRequest {
    oneof request {
        ForwardRequest forward = 1;
        AcquireShardRequest acquire_shard = 2;
        MoveOutRequest move_out = 3;
    }
}

message MoveShardResponse {
    oneof response {
        ForwardResponse forward = 1;
        AcquireShardResponse acquire_shard = 2;
        MoveOutResponse move_out = 3;
    }
}

message ForwardRequest {
    uint64 group_id = 1;
    uint64 shard_id = 2;
    repeated ValueSet forward_data = 3;
    GroupRequestUnion request = 4;
}

message ForwardResponse {
    GroupResponseUnion response = 1;
}

message AcquireShardRequest {
    MoveShardDesc desc = 1;
}

message AcquireShardResponse {}

message MoveOutRequest {
    MoveShardDesc desc = 1;
}

message MoveOutResponse {}
